{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, precision_score, recall_score, f1_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and assign number of CPU cores\n",
    "df = pd.read_csv('../data/creditcard.csv')\n",
    "os.environ[\"LOKY_MAX_CPU_COUNT\"] = \"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the amount feature\n",
    "scaler = StandardScaler()\n",
    "df['Amount_scaled'] = scaler.fit_transform(df[['Amount']])\n",
    "df.drop(['Time', 'Amount'], axis=1, inplace=True)\n",
    "\n",
    "# Split into Features 'X' and 'y' for train-test split\n",
    "X = df.drop('Class', axis=1)\n",
    "y = df['Class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Improve class imblance with SMOTE on training data\n",
    "sm = SMOTE(random_state=42)\n",
    "X_train_sm, y_train_sm = sm.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- LogisticRegression ----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.99     56864\n",
      "           1       0.06      0.92      0.11        98\n",
      "\n",
      "    accuracy                           0.97     56962\n",
      "   macro avg       0.53      0.95      0.55     56962\n",
      "weighted avg       1.00      0.97      0.98     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train Logistic Regression Model\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train_sm, y_train_sm)\n",
    "y_pred = lr.predict(X_test)\n",
    "print(\"---- LogisticRegression ----\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- RandomForest ----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.87      0.83      0.85        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.94      0.91      0.92     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train Random Forest model\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train_sm, y_train_sm)\n",
    "y_pred = rf.predict(X_test)\n",
    "print(\"---- RandomForest ----\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---- XGBClassifier ----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56864\n",
      "           1       0.69      0.87      0.77        98\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.84      0.93      0.88     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train XGBoost Model\n",
    "xgb = XGBClassifier(eval_metric='logloss')\n",
    "xgb.fit(X_train_sm, y_train_sm)\n",
    "y_pred = xgb.predict(X_test)\n",
    "print(\"---- XGBClassifier ----\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic Regression had the highest recall but extremely low precision (0.06), meaning it labeled almost everything as fraud. While it catches most fraud cases, it also misclassifies a lot of legitimate transactions.\n",
    "- Random Forest gave a strong overall result, with high scores in both precision and recall. It did a better job of catching fraud without over-flagging regular activity.\n",
    "- XGBoost performed similarly, with slightly better recall than Random Forest but lower precision. \n",
    "\n",
    "Since this project deals with financial transactions, precision matters more. It’s more important to avoid flagging normal purchases than to catch every single fraud attempt. Based on that, I’ll be using the Random Forest model for the next phase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/fraud_model.pkl']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the Random Forest trained AI model for application use\n",
    "joblib.dump(rf, '../models/fraud_model.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7ff501c7254dffb37adce0853766a5d079451f07f1ba25f8b3612a0f20c9414f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
